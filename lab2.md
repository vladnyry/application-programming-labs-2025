# Лабораторная работа № 2
# Web Scraping

- [Теория](#теория)
  - [csv](#csv)
  - [Создание собственных итераторов](#создание-собственных-итераторов)
  - [icrawler](#icrawler)
  - [requests](#requests)
  - [BeautifulSoup](#beautifulsoup)
  - [Полезные ссылки](#полезные-ссылки)
- [Задание](#задание)
  - [Общее задание](#общее-задание)
  - [Варианты](#варианты)

# Теория

## csv

Для работы с CSV файлами в Python можно использовать встроенный модуль `csv`.

Пример:

```py
import csv

# данные для записи
data = [
    ["Имя", "Возраст", "Город"],  # Заголовки
    ["Алексей", 30, "Москва"],
    ["Мария", 25, "Санкт-Петербург"],
    ["Иван", 22, "Екатеринбург"]
]

# запись данных в CSV файл
with open('data.csv', mode='w', newline='', encoding='utf-8') as file:
    writer = csv.writer(file)
    writer.writerows(data) # запись всех строк

# дозапись в файл
with open('data.csv', mode='a', newline='', encoding='utf-8') as file:
    writer = csv.writer(file)
    writer.writerow(["Ирина", 33, "Сочи"])
```

## Создание собственных итераторов

Итераторы — это объекты, которые позволяют перебирать другие объекты, такие как списки и кортежи, без необходимости обращаться к ним по индексу. Чтобы создать собственный итератор, необходимо в классе реализовать два метода: `__iter__()` и `__next__()`.

- `__iter__()` — возвращает экземпляр итератора. Обычно это просто self.
   
- `__next__()` — возвращает следующий элемент последовательности. Если элементов больше нет, он должен вызвать исключение StopIteration для уведомления о завершении перебора.

Пример создания собственного итератора:

```py
class SimpleIterator:
    def __init__(self, limit):
        self.limit = limit  # ограничение
        self.counter = 0  # счётчик

    def __iter__(self):
        return self

    def __next__(self):
        if self.counter < self.limit:
            self.counter += 1
            return self.counter
        else:
            raise StopIteration

iterator = SimpleIterator(3)

for val in iterator:
    print(val)
```

Результат работы:

```
1
2
3
```


## icrawler

`icrawler` предоставляет удобные инструменты для скачивания изображений из различных интернет-ресурсов. Этот модуль поддерживает несколько источников, таких как Google Images, Bing Images, Flickr и другие.

Установка:

```
pip install icrawler
``` 

Модуль icrawler включает в себя несколько классов, каждый из которых предназначен для работы с определенным источником изображений. Вот несколько ключевых элементов:

1. ImageDownloader: Класс, который будет отвечать за загрузку изображений.

1. Crawlers: Различные классы для осуществления поиска изображений из разных источников, например:
   - GoogleImageCrawler
   - BingImageCrawler
   - FlickrImageCrawler

Пример:
```py
from icrawler.builtin import GoogleImageCrawler

google_crawler = GoogleImageCrawler(storage={'root_dir': 'images'})
google_crawler.crawl(keyword='cat', max_num=100)
```

- `storage` - параметры хранения (папка, формат и т.д.). В данном случае изображения будут сохранены в папке `images`.
- `keyword` - ключевое слово для поиска.
- `max_num` - максимальное количество изображений для загрузки.


## requests
**requests** — это библиотека Python для работы с HTTP-запросами. Она предоставляет простой и удобный способ взаимодействия с веб-ресурсами.

Установка:
```
pip install requests
```

**get** — это основной HTTP-метод для получения данных с сервера. Он используется когда нужно:
- Загрузить веб-страницу
- Получить данные API
- Скачать файл
- Выполнить поиск

Пример:
```py
headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
response = requests.get("https://my-url.com", headers=headers)
if response.ok:
    print(response.content)
```

## BeautifulSoup

**BeautifulSoup** — это библиотека Python для парсинга HTML и XML документов. Она создает дерево объектов из разобранной страницы, что позволяет легко извлекать и манипулировать данными.

Установка:
```
pip install beautifulsoup4
```

Основные возможности:
- Парсинг "грязного" HTML — работает даже с некорректной разметкой
- Простой поиск — мощные методы для нахождения элементов
- Навигация по дереву — перемещение между элементами
- Изменение документа — добавление, удаление, изменение тегов

Пример:
```py
from bs4 import BeautifulSoup
import requests

# Получаем HTML страницу
url = "https://example.com"
response = requests.get(url)
html = response.text

# Создаем объект BeautifulSoup
soup = BeautifulSoup(html, 'html.parser')

# Извлекаем заголовок страницы
title = soup.find('title')
print(f"Заголовок страницы: {title.text}")

# Находим все ссылки
links = soup.find_all('a')
print(f"Найдено ссылок: {len(links)}")

# Выводим текст всех ссылок
for link in links:
    print(link.get_text(), link.get('href'))
```

## Полезные ссылки

- [csv (doc)](https://docs.python.org/3/library/csv.html)
- [Создание итераторов](https://www.pythonforbeginners.com/basics/how-to-create-an-iterator-in-python)
- [icrawler (doc)](https://icrawler.readthedocs.io/en/stable/)
- [requests (doc)](https://requests.readthedocs.io/en/latest/)
- [BeautifulSoup (doc)](https://beautiful-soup-4.readthedocs.io/en/latest/)

# Задание
## Общее задание

Варианты 1-16: 
- Скачайте от 50 до 1000 изображений своего варианта с помощью icrawler. 

Варианты 17-30:
- Скачайте от 50 до 1000 аудиофайлов своего варианта с помощью requests и BeautifulSoup с сайта https://mixkit.co.

Для всех вариантов:
- Составьте аннотацию в виде csv-файла, в котором будет абсолютный и относительный путь к каждому файлу.
- Напишите итератор по путям к файлам - используйте в качестве параметра конструктора файл-аннотации или путь к папке.

Параметры по варианту, путь к папке для сохранения и путь к файлу аннотации необходимо передавать через аргументы командной строки.

## Варианты

1. Скачать изображения по ключевому слову "**cat**", пользователь задаёт диапазон размеров изображения.
1. Скачать изображения по ключевому слову "**rabbit**", пользователь задаёт несколько диапазонов размеров изображения. Количество изображений каждого диапазона размеров одинаковое.
1. Скачать изображения по ключевому слову "**fish**", пользователь задаёт несколько диапазонов размеров изображения. Количество изображений каждого диапазона размеров случайное, но не меньше 1.
1. Скачать полутоновые изображения по ключевому слову "**dog**". 
1. Скачать изображения по ключевому слову "**bird**", пользователь задаёт цвет изображения из списка цветов на выбор: красный, жёлтый, зелёный, синий.
1. Скачать изображения по ключевому слову "**turtle**", пользователь задаёт несколько цветов из списка на выбор. Количество изображений каждого цвета одинаковое.
1. Скачать изображения по ключевому слову "**snake**", пользователь задаёт несколько цветов из списка на выбор. Количество изображений каждого цвета случайное, но не менеее 1.
1. Скачать изображения текущего года по ключевому слову "**monkey**".
1. Скачать изображения по ключевому слову "**horse**", пользователь задаёт диапазон дат, в которые изображение было выложено.
1. Скачать изображения по ключевому слову "**bear**", пользователь задаёт несколько диапазонов дат, в которые изображение было выложено. Количество изображений каждого диапазона дат одинаковое.
1. Скачать изображения по ключевому слову "**bear**", пользователь задаёт несколько диапазонов дат, в которые изображение было выложено. Количество изображений каждого диапазона дат случайное, но не меньше 1.
1. Скачать изображения по нескольким ключевым словам, заданным пользователем. Количество изображений по каждому ключевому слову одинаковое.
1. Скачать изображения по нескольким ключевым словам, заданным пользователем. Количество изображений по каждому ключевому слову случайное, но не менеее 1.
1. Скачать изображения по ключевому слову "**hedgehog**", пользователь задаёт источник для скачивания из списка на выбор (GoogleImageCrawler, BingImageCrawler и т.д.)
1. Скачать изображения по ключевому слову "**cow**", пользователь задаёт количество потоков для скачивания.
1. Скачать изображения по ключевому слову "**pig**", пользователь задаёт ограничение по времени для скачивания. Изображения должны скачиваться пока не закончится время, либо пока не наберётся минимум (50 изображений). В конце вывести затраченное время.


1. Скачать звуки на тему "**природа**".
1. Скачать звуки на тему "**человек**".
1. Скачать звуки на тему "**животные**".
1. Скачать звуки на тему "**транспорт**".
1. Скачать звуки на случайные темы длиной более 10 секунд.
1. Скачать музыку в жанре pop.
1. Скачать музыку в жанрах rock, jazz, R&B, количество композиций каждого жанра одинаковое.
1. Скачать музыку в жанрах country, funk, classical, количество композиций каждого жанра случайное, но не меньше 1.
1. Скачать музыку, исполненную на пианино.
1. Скачать музыку, исполненную на трубе, укулеле и арфе, количество композиций, исполненных на каждом инструменте одинаковое.
1. Скачать музыку, исполненную на флейте, скрипке и барабанах, количество композиций исполненных на каждом случайное, но не меньше 1.
1. Скачать музыку в заданном пользователем диапазоне длительности композиции (в секундах).
1. Скачать музыку со случайным настроением (mood).
1. Скачать музыку по случайному набору тегов (tag).